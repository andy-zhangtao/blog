# 关于go-zero性能的一些想法 - 01引子

## 背景

go-zero官方介绍说，go-zero是一个集成了多种工程最佳实践的web和rpc框架，引入go-zero后下面的一些优势可以开箱即用：

* 轻松获得支撑千万日活服务的稳定性
* 内建级联超时控制、限流、自适应熔断、自适应降载等微服务治理能力，无需配置和额外代码
* 微服务治理中间件可无缝集成到其它现有框架使用
* 极简的 API 描述，一键生成各端代码
* 自动校验客户端请求参数合法性
* 大量微服务治理和并发工具包

(出自[官方介绍](https://go-zero.dev/cn/))

总体来说，go-zero应该是一个不错的应用框架。但在引入go-zero以后却发现在访问量并不高的情况下，却发生了大量503错误。 而在同等访问量情况下，直接使用go sdk却没有出现这个问题。 

本着好奇的心态，就想看看到底是什么问题。

## 症状

大体症状是这样的：这个产品从其他团队接收过来的。技术栈中引入了go-zero，经过容器化以后部署在Kubernetes集群中。每个Pod的资源分配为: CPU: 1C/Memory: 1024Mi。 

服务调用模型是: `Gateway --> API A --> API B --> RPC B --> Redis`。 不要问我为什么这么设计，历史的锅。

![](https://tva1.sinaimg.cn/large/e6c9d24ely1gzkabirl60j21eq07kacs.jpg)

当qps到达一个量级时，会突然出现大量的503错误。 因为集群中启用了业务级别的`HPA`，当出现503时也会同步进行扩容。因此直接在线上环境不容易定位问题，在测试环境中进行压测后发现：当使用jmeter对单Pod(资源配置相同)压测时，300并发量就会出现80%的503错误。 而此时的qps才500+左右。  

换言之，如果想服务稳定运行，那么qps只能限定在100左右！ 太低了！

## 找原因

一般来说，一个服务如果遇到了性能瓶颈，在排除了bug情况下大致是下面几种情况：

+ 自身问题
    1. 资源竞争(CPU竞争、Memory分配)
    2. IO阻塞(磁盘IO读写阻塞、网络IO阻塞)
    3. Block阻塞(锁竞争、并发锁等待)

+ 中间件问题
    1. 中间件阻塞(无可用链接、中间件负载高)

+ 网络问题
    1. 网络拥塞(高延时、高重传)
    2. 路由错误(高丢包率)

+ 操作系统问题
    1. 系统参数设置不合理(TCP参数设置不合理、进程调度参数设置不合理)

在这些可能的原因中，网络问题和操作系统问题可以快速排除，因为这两个问题是全局性问题，如果有问题不会只影响单独某个服务，而应该会一棍子打死一大片。 而当前只有确定某个服务受到影响，所以可以排除这两个原因。 

那么就剩下自身问题和中间问题了，在我刚入行做程序员的时候，那会儿遇到性能问题大多采用`盲猜大法`，就是百度一个可能的原因，然后往症状上面去套，看能不能自圆其说。 百度的质量大家是有目共睹的，所以采用这种方法浪费时间和精力，而且最终收获的经验都是零散不成体系的。 后来看到Brendan Gregg的<性能之巅>和<程序员的自我修养>才算是明白性能问题的由来了，其实定位性能问题也是有方法和规律可循的。 

### 分析

在定位性能问题时，首先需要使用排除法，将不可能的因素第一轮就排除掉。 比如下面的资源竞争问题。

+ 自身问题 - 资源竞争

对于资源竞争问题，可以查看系统参数来定位。 在节点中首先观察CPU和Memory数据，查看当前系统负载怎么样。 

Memory问题容易定位，比如Pod中的容器限定Memory为1024Mi，如果当前memory到达了上限那么会出现两种情况：
1. 不允许OS进行OOM Kill，进程会被block在malloc环节。对外的表现就是处理超时(因为处理过程被Block了，等到外部的超时时间到达后，会自然而然的报错)
2. OS进行OOM Kill，那么容器会被销毁重建。 此时无论是Pod Status还是Dmesg都会留有recreate的记录。

而CPU会复杂一些，假定当前节点CPU是16C。 那么首先观察Load， Load的分布应该有这么三种情况:

|Load值|CPU利用率(加权值)| 指标含义|
|------|-------|-------|
| < 16 |  < 100% | 系统空闲 |
| < 16 |  = 100% | 当前都是CPU密集型任务，并且都运行正常无性能问题 |
| < 16 |  > 100% | 当前都是CPU密集型任务，有轻度CPU竞争问题，但还没有出现性能问题 |
| = 16 |  < 100% | 当前存在CPU密集型任务和IO阻塞型任务，系统压力可控 |
| = 16 |  = 100% | 当前存在CPU密集型任务和IO阻塞型任务，系统存在CPU竞争，需要关注阻塞型任务 |
| = 16 |  > 100% | 当前存在CPU密集型任务和IO阻塞型任务，有CPU竞争情况发生，需要重点关注阻塞型任务 |
| > 16 |  < 100% | 当前存在大量IO阻塞型任务，而且大部分任务都在等待IO，存在性能问题 |
| > 16 |  = 100% | 当前存在大量IO阻塞型任务，并且还有大量的任务切换，存在性能问题 |
| > 16 |  > 100% | 当前存在大量IO阻塞型任务，系统压力非常大 |

登陆到节点中，需要首先判断当前节点情况。 但从症状表象来说，以上这些几乎无用！ 因为只有这个服务出现这种情况，而其他服务并没有出现此类情况，所以不会系统CPU负载引起的。 

那么就不需要检查CPU负载了么？ 不，需要检查。只不过需要检查容器的CPU负载，通过`docker stats`来看指定容器的负载。 可是～～，不能直勾勾的使用`docker stats`来查看，因为`stats`命令是读取的`/proc/pid/stat`文件经过计算得出的CPU使用率，这个数是当时的时点数。只能在故障发生时查看才有意义，当故障都恢复了再查看就没有任何意义了。 

所以CPU问题暂时无法排除，留待后续压测时根据指标数据来判定。

下一步需要排除的是IO阻塞，和CPU问题一个思路。 首先可以排除是节点问题(因为只有一个服务出现问题，并非是全面性的问题)。而且当前查看的是时点数，并非是故障数据，所以也只能等着压测时采集阻塞数据。

可阻塞数据和CPU数据有一些不同的地方，直接通过系统指标只能定性不能定量，也就是只能判定存在IO阻塞，而不能说明是什么类型的IO阻塞。 所以对于IO阻塞问题，采用冷火焰图是一个非常好的判定方式。

最后是Block阻塞，这个Block阻塞如果存在的话，会在CPU指标中反映出来。比如当出现Block阻塞时，会出现CPU利用低并且QPS很低这种数据组合。但这种组合也无法排除第二种IO阻塞的原因。所以对于Block阻塞来说，也建议采用冷火焰来看看到底CPU在等待谁。

+ 中间件问题

排除自身问题之后，就可以将焦点转移到中间件上面了。 对于中间件来说，无非也是普通的进程而已，所以排查思路和上面的思路一摸一样。可中间件和我们的进程也不同，大部分中间件都久经考验，一般不会有明显的性能问题，大部分的性能问题都是业务或者参数引起的。

按照可能的故障原因，可以按照下面的优先级进行排查：

1. 中间件负载，看看是不是有类似于慢查询、计算大的操作所引起的CPU竞争。
2. 中间件数据，热点数据，Miss数据等
3. 检查链接池参数，判定是否因为无可用链接导致。
4. 检查服务到中间的网络质量，是否存在高延时、高丢包率等情况
5. 中间件参数，口令、IP、DB和Table名称，Schema等


对于中间件定位来说也有技巧。 比如服务突然全部故障，那么就找全局性的原因。比如口令、IP、表结构、网络拓扑等。 如果是某些服务可用，某些服务不可用。那么就查和这些故障服务相关的`区域性`数据，例如服务使用的Table、Schema、Hot Data、Miss Data等这些原因。如果是间歇性故障，那么重点检查负载(是不是由于其他服务的慢查询导致的高负载)、网络质量(延时是否突然变高、带宽是否足够)、 链接池(是否到达服务端链接池的上限)

回到本次故障上面，最后`RPC B`调用了`Redis`，所以也无法排除是不是中间件的问题。 所以需要先排除自身问题之后，再来找`Redis`的问题。

在下一篇中，我来展示如何通过`热火焰图`和`冷火焰图`定位和排除自身原因。
