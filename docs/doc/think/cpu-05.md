# 关于go-zero性能的一些想法-05回归问题的本质

书三接上文。 上篇大致聊了一下判断CPU是否存在性能问题的几个方法论。本文会结合go-zero的症状来实际分析一下故障场景。

首先回顾一下使用go-zero的业务模型：

![](https://tva1.sinaimg.cn/large/e6c9d24ely1gzkabirl60j21eq07kacs.jpg)

在这个调用模型中，客户端的请求经过网关直接请求后端API服务A，在A服务中，会再次调用B API，而在B API中又会再次调用RPC服务。 RPC会通过获取Redis数据来进行业务逻辑处理。 

此时服务症状是：当qps到达一个量级时，会突然出现大量的503错误。与此同时，go-zero会显示大量出发熔断的日志: `dropped: x dropreq, cpu: xxxx, maxPass: x, minRt: x.00, hot: false, flying: x, avgFlying: xx `。 大意指的是当前服务触发了系统熔断(go-zero有不同的熔断算法)，当前有x个请求被drop掉了，同时显示一些帮助定位的信息： 当前容器CPU使用率是多少，当前服务最大可以承受的请求数是多少等等这些信息。

通过日志可以确定以下信息：

1. CPU是否发生了竞争(通过CPU: xxxx来判断)
2. 是否是因为服务处理能力过低引起(flying表示当前有多少个在途请求，avgFlying表示当前服务可以承受的请求数)

通过这条日志，可以为下一步的性能定位确定大致的思路。

回到go-zero故障场景中来，当时熔断时CPU并不高，只有20%左右。所以可以排除CPU竞争引起的性能问题，而Flying >> avgFlying。 说明是超过了服务可以处理的请求上限而引起的故障。 

正常情况下，我们也应该采集当前节点的Load来辅助定位。 但很可惜，这个服务运行在Kubernetes之中，单个容器的性能问题不足以引起节点Load有很大的波动。但并非没有办法进行定位。

通过前面几节的铺垫，我们可以知道在正常情况下，CPU应该是尽可能运行用户态代码的，花在用户态上面的CPU时间应该是越高越好。 但此时CPU利用率在20%左右，服务就无法处理更大量的请求，那么就说明当前应该是存在阻塞事件，导致CPU频繁切换，花在内核态和中断调用上面的时间远远大于用户态时间。

所以当前我们的处理思路放在了阻塞IO上面。

**性能定位的几个步骤**

![](https://tva1.sinaimg.cn/large/e6c9d24ely1h00ig8jct6j217206imy7.jpg)

**并不是每一次的怀疑都是准确的，性能分析就是不停的怀疑不停的验证。甚至有的时候，还不等出现怀疑环节时，故障就自行消失了。 所以对待性能问题，一定要有耐心和强大的扛挫折能力。**

现在阻塞IO是我们怀疑的地方，第二步就是使用工具进行定性分析。 何为定性分析，何为定量分析呢？

在性能检测中，定性分析指的是通过论据确定当前逻辑或者当前流程一定存在性能问题，至于是如何引起的，或者是哪行代码引起的，暂时还无法定位。 

而定量分析，则是通过工具确定是具体哪个逻辑，哪行代码所引起的性能问题。 

定性分析用于确定排查方向，而定量分析则用于解决最终的问题。

用于定性分析时常用的工具莫过于火焰图了。 火焰图分为两类： 热火焰和冷火焰。

下图是热火焰(使用golang pprof工具生成)：
![](https://tva1.sinaimg.cn/large/e6c9d24ely1h00ith91j0j21j50u0k5o.jpg)

热火焰强调找到频繁消耗CPU时间的热点函数。 在上图中，横轴表示使用CPU的时间，纵轴表示函数调用链。例如最上面是root，第二行是http.serve。 那么就表示在root函数中，调用了http.serv函数。而第三行的函数则是由http.serve所调用的。 

在查看热火焰图时，不需要关注颜色，不同的颜色仅仅用于区分不同的函数，没有任何其它含义。 

当我们在查看热火焰图时，只需要找横轴宽度最大的函数即可，同时逐个判断这些函数是否可能存在性能瓶颈。 在过滤函数时有几个准则:
1. 第一行可以pass，因为第一行大部分都是入口函数，不会存在性能问题。
2. 大厂商提供的sdk不需要考虑，因为大厂商提供的sdk都经过了丰富的场景验证，基本不会出现性能问题，基本有也是非常低概率的。
3. 找跨度最长的用户业务逻辑函数，这个函数大概率是性能发生点。

按照这个方法再重新审视一遍热火焰图，可以发现有可能是性能瓶颈的函数是`redis.Del.func1`，也就是图中标记为2的函数。 
![](https://tva1.sinaimg.cn/large/e6c9d24ely1h00iuf5ud0j21hx0u0k6k.jpg)

不要被这个函数名称所迷惑，这个函数是go-zero自行实现的一个函数，并非是redis官方函数。所以有可能就是性能瓶颈点。

根据热火焰的分析，我们现在有两个排查思路：
1. 按照标记为2的那块代码去检查为什么Del这么慢。
2. 按照标记为1的那块代码去检查为什么获取链接(`redis.getRedis`)会这么频繁的在使用CPU。

我个人比较倾向排查1，这是因为一般来说，每个服务再同中间件进行链接时都会选择使用链接池。 那么不应该这么高频的再获取链接，所以怀疑go-zero是不是没有使用链接池，或者链接池失效。 因此优先排查一下`redis.getRedis`。

前面也说了，在第一步确定怀疑对象时，怀疑这是因为阻塞IO引起的问题。 如果真的是因为阻塞IO所引起的问题，依靠热火焰图是无法定位的。 因为阻塞IO是不消耗CPU时间的，不会在热火焰图中体现出来。 检测阻塞IO依靠的是冷火焰图。也就是下面的这张图：

![](https://tva1.sinaimg.cn/large/e6c9d24ely1h00j1629khj21tm0s4na5.jpg)

冷火焰图的使用方式和热火焰图的方式一摸一样，横轴表示阻塞CPU的时间，纵轴表示调用链。 哪个函数横轴越宽，就说明其引起的CPU阻塞就越严重。

通过图中可以看出`redis.getConn`所引起的时间最长，那么和热火焰图中`redis.getRedis`好像是可以相互呼应的。 因为go-zero每次操作Reids时都需要调用`redis.getRedis`来获取一个可用链接，而`redis.getRedis`又通过`redis.getConn`来获取真正的链接，此时`redis.getConn`很慢就导致上层应用获取redis链接就很慢，由此就拉低了处理效率，进而引发了故障。

这个结论就是性能检测中的`大胆假设`，紧跟着就需要做`小心求证`了。 如何求证呢？下篇再说如何设计对照实验。